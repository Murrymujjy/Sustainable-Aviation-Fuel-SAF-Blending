{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75eea0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip uninstall pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e666875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in c:\\users\\user\\anaconda3\\lib\\site-packages (1.2.8)\n",
      "Requirement already satisfied: graphviz in c:\\users\\user\\anaconda3\\lib\\site-packages (from catboost) (0.21)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\anaconda3\\lib\\site-packages (from catboost) (3.10.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.16.0 in c:\\users\\user\\appdata\\roaming\\python\\python313\\site-packages (from catboost) (2.3.1)\n",
      "Requirement already satisfied: pandas>=0.24 in c:\\users\\user\\appdata\\roaming\\python\\python313\\site-packages (from catboost) (2.3.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\anaconda3\\lib\\site-packages (from catboost) (1.15.3)\n",
      "Requirement already satisfied: plotly in c:\\users\\user\\anaconda3\\lib\\site-packages (from catboost) (5.24.1)\n",
      "Requirement already satisfied: six in c:\\users\\user\\appdata\\roaming\\python\\python313\\site-packages (from catboost) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\roaming\\python\\python313\\site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\roaming\\python\\python313\\site-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\roaming\\python\\python313\\site-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib->catboost) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib->catboost) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (3.2.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\user\\appdata\\roaming\\python\\python313\\site-packages (from plotly->catboost) (9.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "377af973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 📦 Imports ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "153fea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 📁 Load Data ===\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "196daa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    component_cols = [f'C{i}_' for i in range(1, 6)]\n",
    "    property_names = ['Property_1', 'Property_2', 'Property_3', 'Property_4', 'Property_5',\n",
    "                      'Property_6', 'Property_7', 'Property_8', 'Property_9', 'Property_10']\n",
    "\n",
    "    # 1. Weighted average of each property\n",
    "    for prop in property_names:\n",
    "        weighted_sum = sum(df[f'C{i}_wt%'] * df[f'C{i}_{prop}'] for i in range(1, 6))\n",
    "        df[f'Weighted_{prop}'] = weighted_sum\n",
    "\n",
    "    # 2. Component-wise mean & std of properties\n",
    "    for i in range(1, 6):\n",
    "        props = [df[f'C{i}_{p}'] for p in property_names]\n",
    "        df[f'C{i}_mean'] = np.mean(props, axis=0)\n",
    "        df[f'C{i}_std'] = np.std(props, axis=0)\n",
    "\n",
    "    # 3. Total component weight (should be 100 but for safety)\n",
    "    df['Total_wt'] = df[[f'C{i}_wt%' for i in range(1, 6)]].sum(axis=1)\n",
    "\n",
    "    # 4. Normalized weights\n",
    "    for i in range(1, 6):\n",
    "        df[f'C{i}_wt_norm'] = df[f'C{i}_wt%'] / df['Total_wt']\n",
    "\n",
    "    # 5. Weighted mean & std of each property across components\n",
    "    for prop in property_names:\n",
    "        values = np.stack([df[f'C{i}_{prop}'] for i in range(1, 6)], axis=1)\n",
    "        weights = np.stack([df[f'C{i}_wt_norm'] for i in range(1, 6)], axis=1)\n",
    "        df[f'WeightedMean_{prop}'] = np.sum(values * weights, axis=1)\n",
    "        df[f'WeightedStd_{prop}'] = np.std(values * weights, axis=1)\n",
    "\n",
    "    # 6. Property Interaction (pairwise product of important properties)\n",
    "    important_props = ['Property_1', 'Property_4', 'Property_6', 'Property_9']\n",
    "    for i in range(len(important_props)):\n",
    "        for j in range(i + 1, len(important_props)):\n",
    "            prop1, prop2 = important_props[i], important_props[j]\n",
    "            df[f'Interact_{prop1}_{prop2}'] = df[f'Weighted_{prop1}'] * df[f'Weighted_{prop2}']\n",
    "\n",
    "    # 7. Deviation of each component from blend-weighted property mean\n",
    "    for prop in property_names:\n",
    "        weighted = df[f'Weighted_{prop}']\n",
    "        for i in range(1, 6):\n",
    "            df[f'C{i}_{prop}_dev'] = df[f'C{i}_{prop}'] - weighted\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46205d29",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'C1_wt%'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'C1_wt%'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m target_cols \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m train\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m col\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Apply feature engineering to full dataset\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m X \u001b[38;5;241m=\u001b[39m engineer_features(train\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mtarget_cols))\n\u001b[0;32m     12\u001b[0m y \u001b[38;5;241m=\u001b[39m train[target_cols]\n\u001b[0;32m     13\u001b[0m X_test_fe \u001b[38;5;241m=\u001b[39m engineer_features(test)\n",
      "Cell \u001b[1;32mIn[10], line 13\u001b[0m, in \u001b[0;36mengineer_features\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 1. Weighted average of each property\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prop \u001b[38;5;129;01min\u001b[39;00m property_names:\n\u001b[1;32m---> 13\u001b[0m     weighted_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_wt%\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprop\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m     14\u001b[0m     df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeighted_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprop\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m weighted_sum\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 2. Component-wise mean & std of properties\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 13\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 1. Weighted average of each property\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prop \u001b[38;5;129;01min\u001b[39;00m property_names:\n\u001b[1;32m---> 13\u001b[0m     weighted_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_wt%\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprop\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m     14\u001b[0m     df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeighted_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprop\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m weighted_sum\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 2. Component-wise mean & std of properties\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\frame.py:4107\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4107\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4109\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3817\u001b[0m     ):\n\u001b[0;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'C1_wt%'"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "# Define target columns\n",
    "target_cols = [col for col in train.columns if col.startswith('target_')]\n",
    "\n",
    "# Apply feature engineering to full dataset\n",
    "X = engineer_features(train.drop(columns=target_cols))\n",
    "y = train[target_cols]\n",
    "X_test_fe = engineer_features(test)\n",
    "\n",
    "# K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "val_scores = []\n",
    "test_preds = np.zeros((len(test), len(target_cols)))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    model = MultiOutputRegressor(LGBMRegressor(\n",
    "        n_estimators=1000, \n",
    "        learning_rate=0.05,\n",
    "        random_state=fold,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "    \n",
    "    model.fit(X_tr, y_tr)\n",
    "    val_pred = model.predict(X_val)\n",
    "    fold_score = mean_absolute_percentage_error(y_val, val_pred)\n",
    "    val_scores.append(fold_score)\n",
    "\n",
    "    print(f\"Fold {fold+1} MAPE: {fold_score:.4f}\")\n",
    "\n",
    "    test_preds += model.predict(X_test_fe) / kf.n_splits\n",
    "\n",
    "# Show average validation score\n",
    "print(f\"\\nAverage CV MAPE: {np.mean(val_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccc3876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 📁 Load Data ===\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Remove ID if exists\n",
    "test_ids = test_df[\"ID\"] if \"ID\" in test_df.columns else test_df.index\n",
    "train_df.drop(columns=[\"ID\"], inplace=True, errors='ignore')\n",
    "test_df.drop(columns=[\"ID\"], inplace=True, errors='ignore')\n",
    "\n",
    "# 🎯 Target Columns\n",
    "target_cols = [f\"BlendProperty{i}\" for i in range(1, 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea12ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted averages of component properties\n",
    "for i in range(1, 11):\n",
    "    train_df[f\"WeightedAvg_Property{i}\"] = sum(\n",
    "        train_df[f\"Component{j}_fraction\"] * train_df[f\"Component{j}_Property{i}\"] for j in range(1, 6)\n",
    "    )\n",
    "    test_df[f\"WeightedAvg_Property{i}\"] = sum(\n",
    "        test_df[f\"Component{j}_fraction\"] * test_df[f\"Component{j}_Property{i}\"] for j in range(1, 6)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359d444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended SHAP-based Top 15 Features\n",
    "top_features = [\n",
    "    \"Component5_fraction\", \"WeightedAvg_Property1\", \"Component2_fraction\", \n",
    "    \"Component4_fraction\", \"Component3_fraction\", \"Component1_fraction\",\n",
    "    \"Component3_Property1\", \"Component2_Property1\", \"Component4_Property1\",\n",
    "    \"WeightedAvg_Property2\", \"Component1_Property1\", \"Component5_Property1\",\n",
    "    \"WeightedAvg_Property3\", \"Component2_Property2\", \"WeightedAvg_Property4\"\n",
    "]\n",
    "\n",
    "X = train_df[top_features]\n",
    "y = train_df[target_cols]\n",
    "X_test = test_df[top_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaed964",
   "metadata": {},
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cat_preds = np.zeros((X_test.shape[0], len(target_cols)))\n",
    "lgb_preds = np.zeros((X_test.shape[0], len(target_cols)))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\n📂 Fold {fold + 1}\")\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    # 🐈 CatBoost (custom tuned)\n",
    "    cat_model = MultiOutputRegressor(\n",
    "        CatBoostRegressor(\n",
    "            iterations=1500,\n",
    "            learning_rate=0.015,   # Reduced LR for stability\n",
    "            depth=9,\n",
    "            l2_leaf_reg=4,\n",
    "            loss_function=\"MAE\",\n",
    "            verbose=0,\n",
    "            random_seed=42,\n",
    "            early_stopping_rounds=75\n",
    "        )\n",
    "    )\n",
    "    cat_model.fit(X_train, y_train)\n",
    "    cat_preds += cat_model.predict(X_test)\n",
    "\n",
    "    # 🌱 LightGBM (custom tuned)\n",
    "    lgb_model = MultiOutputRegressor(\n",
    "        LGBMRegressor(\n",
    "            n_estimators=1500,\n",
    "            learning_rate=0.01,\n",
    "            max_depth=7,\n",
    "            num_leaves=60,\n",
    "            subsample=0.75,\n",
    "            colsample_bytree=0.7,\n",
    "            reg_lambda=1.5,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    )\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    lgb_preds += lgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415d0b2e",
   "metadata": {},
   "source": [
    "cat_preds /= kf.get_n_splits()\n",
    "lgb_preds /= kf.get_n_splits()\n",
    "\n",
    "final_preds = 100.85 * cat_preds + 0.25 * lgb_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2937c41e",
   "metadata": {},
   "source": [
    "submission = pd.DataFrame(final_preds, columns=target_cols)\n",
    "submission.insert(0, \"ID\", test_ids)\n",
    "submission.to_csv(\"submission_cat_lgb25_extended.csv\", index=False)\n",
    "\n",
    "print(\"\\n✅ submission_cat75_lgb25_extended.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795dffe9-e586-4a3d-a3be-2991b57a24fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The highest code\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# === Load Data ===\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# === Handle ID Column ===\n",
    "test_ids = test_df[\"ID\"] if \"ID\" in test_df.columns else test_df.index\n",
    "train_df.drop(columns=[\"ID\"], inplace=True, errors='ignore')\n",
    "test_df.drop(columns=[\"ID\"], inplace=True, errors='ignore')\n",
    "\n",
    "# === Separate Features and Targets ===\n",
    "target_cols = [f\"BlendProperty{i}\" for i in range(1, 11)]\n",
    "X = train_df.drop(columns=target_cols)\n",
    "y = train_df[target_cols]\n",
    "X_test = test_df.copy()\n",
    "\n",
    "# === Feature Engineering: Weighted Average Properties ===\n",
    "for i in range(1, 11):  # Property1 to Property10\n",
    "    weighted_train = sum(\n",
    "        train_df[f\"Component{j}_fraction\"] * train_df[f\"Component{j}_Property{i}\"] for j in range(1, 6)\n",
    "    )\n",
    "    weighted_test = sum(\n",
    "        test_df[f\"Component{j}_fraction\"] * test_df[f\"Component{j}_Property{i}\"] for j in range(1, 6)\n",
    "    )\n",
    "    X[f\"WeightedAvg_Property{i}\"] = weighted_train\n",
    "    X_test[f\"WeightedAvg_Property{i}\"] = weighted_test\n",
    "\n",
    "# === Setup K-Fold ===\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "lgb_preds = np.zeros((X_test.shape[0], y.shape[1]))\n",
    "cat_preds = np.zeros((X_test.shape[0], y.shape[1]))\n",
    "\n",
    "# === Cross-Validation Loop ===\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\n🔁 Fold {fold+1}\")\n",
    "\n",
    "    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    # === LightGBM Model ===\n",
    "    lgb_model = MultiOutputRegressor(\n",
    "        LGBMRegressor(\n",
    "            n_estimators=500,\n",
    "            learning_rate=0.03,\n",
    "            max_depth=8,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    )\n",
    "    lgb_model.fit(X_tr, y_tr)\n",
    "    lgb_preds += lgb_model.predict(X_test)\n",
    "\n",
    "    # === CatBoost Model ===\n",
    "    cat_model = MultiOutputRegressor(\n",
    "        CatBoostRegressor(\n",
    "            iterations=500,\n",
    "            learning_rate=0.03,\n",
    "            depth=8,\n",
    "            loss_function=\"MAE\",\n",
    "            verbose=0,\n",
    "            random_seed=42\n",
    "        )\n",
    "    )\n",
    "    cat_model.fit(X_tr, y_tr)\n",
    "    cat_preds += cat_model.predict(X_test)\n",
    "\n",
    "# === Average Over Folds ===\n",
    "lgb_preds /= kf.get_n_splits()\n",
    "cat_preds /= kf.get_n_splits()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c6104d",
   "metadata": {},
   "source": [
    "# === Weighted Blending ===\n",
    "final_preds = 0.9 * lgb_preds + 0.3 * cat_preds\n",
    "\n",
    "# === Create Submission ===\n",
    "submission = pd.DataFrame(final_preds, columns=target_cols)\n",
    "submission.insert(0, \"ID\", test_ids)\n",
    "submission.to_csv(\"submission_lgb_cat_blend1.csv\", index=False)\n",
    "print(\"\\n✅ submission_lgb_cat_blend1.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3556cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ee36de",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a5e8f7",
   "metadata": {},
   "source": [
    "# === HGBR Pipeline ===\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "hgbr_preds = np.zeros((X_test.shape[0], len(target_cols)))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\n🌲 Fold {fold + 1} - HistGradientBoostingRegressor\")\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    hgbr_model = MultiOutputRegressor(\n",
    "        HistGradientBoostingRegressor(\n",
    "            loss=\"absolute_error\",\n",
    "            learning_rate=0.05,\n",
    "            max_iter=800,\n",
    "            max_depth=6,\n",
    "            l2_regularization=0.1,\n",
    "            random_state=42\n",
    "        )\n",
    "    )\n",
    "\n",
    "    hgbr_model.fit(X_train, y_train)\n",
    "    hgbr_preds += hgbr_model.predict(X_test)\n",
    "\n",
    "hgbr_preds /= kf.get_n_splits()\n",
    "\n",
    "# Save predictions\n",
    "submission_hgbr = pd.DataFrame(hgbr_preds, columns=target_cols)\n",
    "submission_hgbr.insert(0, \"ID\", test_ids)\n",
    "submission_hgbr.to_csv(\"submission_hgbr.csv\", index=False)\n",
    "\n",
    "print(\"✅ submission_hgbr.csv saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425ecccd",
   "metadata": {},
   "source": [
    "# === 📦 Imports ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# === 📁 Load Data ===\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# === 🧹 Remove ID if exists ===\n",
    "test_ids = test_df[\"ID\"] if \"ID\" in test_df.columns else test_df.index\n",
    "train_df.drop(columns=[\"ID\"], inplace=True, errors='ignore')\n",
    "test_df.drop(columns=[\"ID\"], inplace=True, errors='ignore')\n",
    "\n",
    "# === 🎯 Target Columns ===\n",
    "target_cols = [f\"BlendProperty{i}\" for i in range(1, 11)]\n",
    "\n",
    "# === 🧠 Feature Engineering: Weighted Averages ===\n",
    "for i in range(1, 11):\n",
    "    train_df[f\"WeightedAvg_Property{i}\"] = sum(\n",
    "        train_df[f\"Component{j}_fraction\"] * train_df[f\"Component{j}_Property{i}\"] for j in range(1, 6)\n",
    "    )\n",
    "    test_df[f\"WeightedAvg_Property{i}\"] = sum(\n",
    "        test_df[f\"Component{j}_fraction\"] * test_df[f\"Component{j}_Property{i}\"] for j in range(1, 6)\n",
    "    )\n",
    "\n",
    "# === ✅ Top SHAP-based Features Only ===\n",
    "top_features = [\n",
    "    \"Component5_fraction\", \"WeightedAvg_Property1\", \"Component2_fraction\", \n",
    "    \"Component4_fraction\", \"Component3_fraction\", \"Component1_fraction\",\n",
    "    \"Component3_Property1\", \"Component2_Property1\", \"Component4_Property1\"\n",
    "]\n",
    "\n",
    "X = train_df[top_features]\n",
    "y = train_df[target_cols]\n",
    "X_test = test_df[top_features]\n",
    "\n",
    "# === 🔁 Cross Validation Setup ===\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cat_preds = np.zeros((X_test.shape[0], len(target_cols)))\n",
    "hgb_preds = np.zeros((X_test.shape[0], len(target_cols)))\n",
    "\n",
    "# === 🔁 Train per Fold ===\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\n📂 Fold {fold + 1}\")\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    # 🐈 CatBoost\n",
    "    cat_model = MultiOutputRegressor(\n",
    "        CatBoostRegressor(\n",
    "            iterations=1500,\n",
    "            learning_rate=0.015,\n",
    "            depth=9,\n",
    "            l2_leaf_reg=4,\n",
    "            loss_function=\"MAE\",\n",
    "            verbose=0,\n",
    "            random_seed=42,\n",
    "            early_stopping_rounds=75\n",
    "        )\n",
    "    )\n",
    "    cat_model.fit(X_train, y_train)\n",
    "    cat_preds += cat_model.predict(X_test)\n",
    "\n",
    "    # 🌄 HistGradientBoostingRegressor\n",
    "    hgb_model = MultiOutputRegressor(\n",
    "        HistGradientBoostingRegressor(\n",
    "            max_iter=1000,\n",
    "            learning_rate=0.02,\n",
    "            max_depth=8,\n",
    "            l2_regularization=1.0,\n",
    "            early_stopping=True,\n",
    "            random_state=42\n",
    "        )\n",
    "    )\n",
    "    hgb_model.fit(X_train, y_train)\n",
    "    hgb_preds += hgb_model.predict(X_test)\n",
    "\n",
    "# === 📊 Average Predictions Over Folds ===\n",
    "cat_preds /= kf.get_n_splits()\n",
    "hgb_preds /= kf.get_n_splits()\n",
    "\n",
    "# === ⚖️ Final Weighted Blend (CatBoost 70%, HGBR 30%)\n",
    "final_preds = 0.7 * cat_preds + 0.3 * hgb_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763ef21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 💾 Submission ===\n",
    "submission = pd.DataFrame(final_preds, columns=target_cols)\n",
    "submission.insert(0, \"ID\", test_ids)\n",
    "submission.to_csv(\"submission_cat70_hgbr30.csv\", index=False)\n",
    "\n",
    "print(\"\\n✅ submission_cat70_hgbr30.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aadb46",
   "metadata": {},
   "source": [
    "# === 📦 Imports ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# === 📁 Load Data ===\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# === 🎯 Targets & Features ===\n",
    "target_cols = [f\"BlendProperty{i}\" for i in range(1, 11)]\n",
    "test_ids = test_df[\"ID\"] if \"ID\" in test_df.columns else test_df.index\n",
    "train_df.drop(columns=[\"ID\"], inplace=True, errors=\"ignore\")\n",
    "test_df.drop(columns=[\"ID\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "# === 🧠 Feature Engineering: Weighted Averages ===\n",
    "for i in range(1, 11):\n",
    "    train_df[f\"WeightedAvg_Property{i}\"] = sum(\n",
    "        train_df[f\"Component{j}_fraction\"] * train_df[f\"Component{j}_Property{i}\"] for j in range(1, 5+1)\n",
    "    )\n",
    "    test_df[f\"WeightedAvg_Property{i}\"] = sum(\n",
    "        test_df[f\"Component{j}_fraction\"] * test_df[f\"Component{j}_Property{i}\"] for j in range(1, 5+1)\n",
    "    )\n",
    "\n",
    "# === ✅ SHAP-based Top Features ===\n",
    "top_features = [\n",
    "    \"Component5_fraction\", \"WeightedAvg_Property1\", \"Component2_fraction\", \n",
    "    \"Component4_fraction\", \"Component3_fraction\", \"Component1_fraction\",\n",
    "    \"Component3_Property1\", \"Component2_Property1\", \"Component4_Property1\"\n",
    "]\n",
    "\n",
    "X = train_df[top_features]\n",
    "y = train_df[target_cols]\n",
    "X_test = test_df[top_features]\n",
    "\n",
    "# === ⚖️ Standardize Features for MLP ===\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# === 🔁 K-Fold CV ===\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "mlp_preds = np.zeros((X_test.shape[0], len(target_cols)))\n",
    "cat_preds = np.zeros((X_test.shape[0], len(target_cols)))\n",
    "\n",
    "# === 🔁 Training Loop ===\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\n📂 Fold {fold+1}\")\n",
    "    \n",
    "    X_train_scaled, X_val_scaled = X_scaled[train_idx], X_scaled[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    # === 🤖 MLP Model\n",
    "    mlp_model = MultiOutputRegressor(\n",
    "        MLPRegressor(\n",
    "            hidden_layer_sizes=(100, 80),\n",
    "            activation='relu',\n",
    "            solver='adam',\n",
    "            alpha=0.001,\n",
    "            learning_rate='adaptive',\n",
    "            max_iter=1000,\n",
    "            early_stopping=True,\n",
    "            random_state=42\n",
    "        )\n",
    "    )\n",
    "    mlp_model.fit(X_train_scaled, y_train)\n",
    "    mlp_preds += mlp_model.predict(X_test_scaled)\n",
    "\n",
    "    # === 🐈 CatBoost (for comparison/blending)\n",
    "    cat_model = MultiOutputRegressor(\n",
    "        CatBoostRegressor(\n",
    "            iterations=1500,\n",
    "            learning_rate=0.015,\n",
    "            depth=9,\n",
    "            l2_leaf_reg=4,\n",
    "            loss_function=\"MAE\",\n",
    "            verbose=0,\n",
    "            random_seed=42,\n",
    "            early_stopping_rounds=75\n",
    "        )\n",
    "    )\n",
    "    cat_model.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
    "    cat_preds += cat_model.predict(X_test)\n",
    "\n",
    "# === 📊 Average Predictions ===\n",
    "mlp_preds /= kf.get_n_splits()\n",
    "cat_preds /= kf.get_n_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98919ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 🧪 Blend CatBoost + MLP (optional)\n",
    "final_preds = 0.9 * cat_preds + 0.0 * mlp_preds\n",
    "\n",
    "# === 💾 Save Submission ===\n",
    "submission = pd.DataFrame(final_preds, columns=target_cols)\n",
    "submission.insert(0, \"ID\", test_ids)\n",
    "submission.to_csv(\"submission_cat80_mlp20.csv\", index=False)\n",
    "\n",
    "print(\"\\n✅ submission_cat80_mlp20.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee43a7c",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# === Load Data ===\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "target_cols = [f\"BlendProperty{i}\" for i in range(1, 11)]\n",
    "\n",
    "# Feature Engineering\n",
    "for i in range(1, 11):\n",
    "    train_df[f\"WeightedAvg_Property{i}\"] = sum(\n",
    "        train_df[f\"Component{j}_fraction\"] * train_df[f\"Component{j}_Property{i}\"] for j in range(1, 6)\n",
    "    )\n",
    "    test_df[f\"WeightedAvg_Property{i}\"] = sum(\n",
    "        test_df[f\"Component{j}_fraction\"] * test_df[f\"Component{j}_Property{i}\"] for j in range(1, 6)\n",
    "    )\n",
    "\n",
    "top_features = [\n",
    "    \"Component5_fraction\", \"WeightedAvg_Property1\", \"Component2_fraction\", \n",
    "    \"Component4_fraction\", \"Component3_fraction\", \"Component1_fraction\",\n",
    "    \"Component3_Property1\", \"Component2_Property1\", \"Component4_Property1\"\n",
    "]\n",
    "\n",
    "X = train_df[top_features]\n",
    "y = train_df[target_cols]\n",
    "X_test = test_df[top_features]\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cat_preds = np.zeros((X_test.shape[0], len(target_cols)))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\n📂 Fold {fold + 1}\")\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    cat_model = MultiOutputRegressor(\n",
    "        CatBoostRegressor(\n",
    "            iterations=1500,\n",
    "            learning_rate=0.015,\n",
    "            depth=9,\n",
    "            l2_leaf_reg=4,\n",
    "            loss_function=\"MAE\",\n",
    "            verbose=0,\n",
    "            random_seed=42,\n",
    "            early_stopping_rounds=75\n",
    "        )\n",
    "    )\n",
    "    cat_model.fit(X_train, y_train)\n",
    "    cat_preds += cat_model.predict(X_test)\n",
    "\n",
    "# Average over folds\n",
    "cat_preds /= kf.get_n_splits()\n",
    "\n",
    "# Save submission\n",
    "submission = pd.DataFrame(cat_preds, columns=target_cols)\n",
    "submission.insert(0, \"ID\", test_df[\"ID\"])\n",
    "submission.to_csv(\"submission_catboost_only_recovery.csv\", index=False)\n",
    "\n",
    "print(\"\\n✅ submission_catboost_only_recovery.csv saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00d71b1",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === 1. Load the submissions ===\n",
    "path_best = r\"C:\\Users\\USER\\Desktop\\shell.ai\\submission_lgb_cat_blend1.csv\"\n",
    "path_bad = r\"C:\\Users\\USER\\Desktop\\shell.ai\\submission_catboost_only_optimized.csv\"      # 10 score\n",
    "\n",
    "# Replace paths with actual ones if different\n",
    "best_df = pd.read_csv(path_best)\n",
    "bad_df = pd.read_csv(path_bad)\n",
    "\n",
    "# === 2. Ensure matching ID and shape ===\n",
    "assert best_df.shape == bad_df.shape, \"Submissions have different shapes!\"\n",
    "assert (best_df['ID'] == bad_df['ID']).all(), \"Mismatch in IDs!\"\n",
    "\n",
    "# === 3. Compute absolute and relative differences ===\n",
    "diff_df = best_df.copy()\n",
    "target_cols = [col for col in best_df.columns if col != \"ID\"]\n",
    "\n",
    "for col in target_cols:\n",
    "    diff_df[col] = np.abs(best_df[col] - bad_df[col])\n",
    "\n",
    "# === 4. Show a few example rows of differences ===\n",
    "print(\"🔍 Sample of absolute differences between submissions:\")\n",
    "display(diff_df.head())\n",
    "\n",
    "# === 5. Mean Difference per Column ===\n",
    "mean_diffs = diff_df[target_cols].mean().sort_values(ascending=False)\n",
    "print(\"\\n📊 Mean absolute difference per target column:\")\n",
    "display(mean_diffs)\n",
    "\n",
    "# === 6. Plot the differences per blend property ===\n",
    "plt.figure(figsize=(10, 5))\n",
    "mean_diffs.plot(kind=\"bar\", color=\"skyblue\", edgecolor=\"black\")\n",
    "plt.title(\"Mean Absolute Difference per Blend Property\")\n",
    "plt.ylabel(\"Mean Absolute Difference\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e254a77",
   "metadata": {},
   "source": [
    "# === 📦 Imports ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# === 📁 Load Data ===\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# === 🧹 Remove ID if present ===\n",
    "test_ids = test_df[\"ID\"] if \"ID\" in test_df.columns else test_df.index\n",
    "train_df.drop(columns=[\"ID\"], inplace=True, errors='ignore')\n",
    "test_df.drop(columns=[\"ID\"], inplace=True, errors='ignore')\n",
    "\n",
    "# === 🎯 Target Columns ===\n",
    "target_cols = [f\"BlendProperty{i}\" for i in range(1, 11)]\n",
    "\n",
    "# === ⚙️ Weighted Average Features ===\n",
    "for i in range(1, 11):\n",
    "    train_df[f\"WeightedAvg_Property{i}\"] = sum(\n",
    "        train_df[f\"Component{j}_fraction\"] * train_df[f\"Component{j}_Property{i}\"] for j in range(1, 6)\n",
    "    )\n",
    "    test_df[f\"WeightedAvg_Property{i}\"] = sum(\n",
    "        test_df[f\"Component{j}_fraction\"] * test_df[f\"Component{j}_Property{i}\"] for j in range(1, 6)\n",
    "    )\n",
    "\n",
    "# === ✅ Top SHAP Features (Based on best run) ===\n",
    "top_features = [\n",
    "    \"Component5_fraction\", \"WeightedAvg_Property1\", \"Component2_fraction\", \n",
    "    \"Component4_fraction\", \"Component3_fraction\", \"Component1_fraction\",\n",
    "    \"Component3_Property1\", \"Component2_Property1\", \"Component4_Property1\"\n",
    "]\n",
    "\n",
    "X = train_df[top_features]\n",
    "y = train_df[target_cols]\n",
    "X_test = test_df[top_features]\n",
    "\n",
    "# === 🔀 K-Fold CV ===\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cat_preds = np.zeros((X_test.shape[0], len(target_cols)))\n",
    "lgb_preds = np.zeros((X_test.shape[0], len(target_cols)))\n",
    "\n",
    "# === 🔁 Train Loop ===\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\n📂 Fold {fold + 1}\")\n",
    "\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    # === 🐈 CatBoost\n",
    "    cat_model = MultiOutputRegressor(\n",
    "        CatBoostRegressor(\n",
    "            iterations=1600,\n",
    "            learning_rate=0.015,\n",
    "            depth=9,\n",
    "            l2_leaf_reg=4.5,\n",
    "            loss_function=\"MAE\",\n",
    "            verbose=0,\n",
    "            random_seed=42,\n",
    "            early_stopping_rounds=75\n",
    "        )\n",
    "    )\n",
    "    cat_model.fit(X_train, y_train)\n",
    "    cat_preds += cat_model.predict(X_test)\n",
    "\n",
    "    # === 🌱 LightGBM\n",
    "    lgb_model = MultiOutputRegressor(\n",
    "        LGBMRegressor(\n",
    "            n_estimators=1200,\n",
    "            learning_rate=0.012,\n",
    "            max_depth=7,\n",
    "            num_leaves=60,\n",
    "            subsample=0.75,\n",
    "            colsample_bytree=0.7,\n",
    "            reg_lambda=1.3,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    )\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    lgb_preds += lgb_model.predict(X_test)\n",
    "\n",
    "# === 📊 Average Fold Predictions ===\n",
    "cat_preds /= kf.get_n_splits()\n",
    "lgb_preds /= kf.get_n_splits()\n",
    "\n",
    "# === ⚖️ Final Blend: CatBoost 80%, LGBM 20%\n",
    "final_preds = 0.80 * cat_preds + 0.20 * lgb_preds\n",
    "\n",
    "# === 💾 Submission\n",
    "submission = pd.DataFrame(final_preds, columns=target_cols)\n",
    "submission.insert(0, \"ID\", test_ids)\n",
    "submission.to_csv(\"submission_cat80_lgb20_final.csv\", index=False)\n",
    "\n",
    "print(\"\\n✅ submission_cat80_lgb20_final.csv created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffcf288",
   "metadata": {},
   "source": [
    "# === 📦 Imports ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# === 📁 Load Data ===\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# === 🧹 Preprocessing ===\n",
    "test_ids = test_df[\"ID\"] if \"ID\" in test_df.columns else test_df.index\n",
    "train_df.drop(columns=[\"ID\"], inplace=True, errors='ignore')\n",
    "test_df.drop(columns=[\"ID\"], inplace=True, errors='ignore')\n",
    "\n",
    "target_cols = [f\"BlendProperty{i}\" for i in range(1, 11)]\n",
    "\n",
    "# === 🧠 Feature Engineering ===\n",
    "for i in range(1, 11):\n",
    "    train_df[f\"WeightedAvg_Property{i}\"] = sum(\n",
    "        train_df[f\"Component{j}_fraction\"] * train_df[f\"Component{j}_Property{i}\"] for j in range(1, 6)\n",
    "    )\n",
    "    test_df[f\"WeightedAvg_Property{i}\"] = sum(\n",
    "        test_df[f\"Component{j}_fraction\"] * test_df[f\"Component{j}_Property{i}\"] for j in range(1, 6)\n",
    "    )\n",
    "\n",
    "top_features = [\n",
    "    \"Component5_fraction\", \"WeightedAvg_Property1\", \"Component2_fraction\", \n",
    "    \"Component4_fraction\", \"Component3_fraction\", \"Component1_fraction\",\n",
    "    \"Component3_Property1\", \"Component2_Property1\", \"Component4_Property1\"\n",
    "]\n",
    "\n",
    "X = train_df[top_features]\n",
    "y = train_df[target_cols]\n",
    "X_test = test_df[top_features]\n",
    "\n",
    "# === 🧠 Models ===\n",
    "cat_model = MultiOutputRegressor(CatBoostRegressor(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.02,\n",
    "    depth=8,\n",
    "    loss_function=\"MAE\",\n",
    "    verbose=0,\n",
    "    random_seed=42\n",
    "))\n",
    "\n",
    "lgb_model = MultiOutputRegressor(LGBMRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.02,\n",
    "    max_depth=7,\n",
    "    subsample=0.75,\n",
    "    colsample_bytree=0.7,\n",
    "    random_state=42\n",
    "))\n",
    "\n",
    "ridge_model = MultiOutputRegressor(RidgeCV(alphas=[0.1, 1.0, 10.0]))\n",
    "\n",
    "# === 🔁 KFold CV ===\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cat_preds = np.zeros((X_test.shape[0], len(target_cols)))\n",
    "lgb_preds = np.zeros((X_test.shape[0], len(target_cols)))\n",
    "ridge_preds = np.zeros((X_test.shape[0], len(target_cols)))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\n📂 Fold {fold + 1}\")\n",
    "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    \n",
    "    cat_model.fit(X_train, y_train)\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    ridge_model.fit(X_train, y_train)\n",
    "    \n",
    "    cat_preds += cat_model.predict(X_test)\n",
    "    lgb_preds += lgb_model.predict(X_test)\n",
    "    ridge_preds += ridge_model.predict(X_test)\n",
    "\n",
    "# === 📊 Average over folds ===\n",
    "cat_preds /= kf.get_n_splits()\n",
    "lgb_preds /= kf.get_n_splits()\n",
    "ridge_preds /= kf.get_n_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f896777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ⚖️ Weighted Blending ===\n",
    "alpha, beta, gamma = 0.6, 0.09, 1.0  # catboost, lgbm, ridge\n",
    "final_preds = alpha * cat_preds + beta * lgb_preds + gamma * ridge_preds\n",
    "\n",
    "# === 💾 Save Submission ===\n",
    "submission = pd.DataFrame(final_preds, columns=target_cols)\n",
    "submission.insert(0, \"ID\", test_ids)\n",
    "submission.to_csv(\"submission_blended_cat_lgb_ridge.csv\", index=False)\n",
    "\n",
    "print(\"\\n✅ Blended submission saved: submission_blended_cat_lgb_ridge.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3c22c0",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Extract features and target\n",
    "target_cols = [col for col in train.columns if \"BlendProperty\" in col]\n",
    "X = train.drop(columns=[\"ID\"] + target_cols)\n",
    "y = train[target_cols]\n",
    "X_test = test.drop(columns=[\"ID\"])\n",
    "\n",
    "# Fill missing values and ensure numeric\n",
    "X = X.fillna(0).astype(float)\n",
    "X_test = X_test.fillna(0).astype(float)\n",
    "y = y.fillna(0).astype(float)\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# KFold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "svr_preds = np.zeros((X_test.shape[0], len(target_cols)))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_scaled)):\n",
    "    X_train, y_train = X_scaled[train_idx], y.iloc[train_idx]\n",
    "\n",
    "    svr = MultiOutputRegressor(SVR(kernel='rbf', C=1.0, epsilon=0.1))\n",
    "    svr.fit(X_train, y_train)\n",
    "    svr_preds += svr.predict(X_test_scaled)\n",
    "\n",
    "# Average predictions\n",
    "svr_preds /= kf.get_n_splits()\n",
    "\n",
    "# Prepare submission\n",
    "submission = pd.DataFrame(svr_preds, columns=target_cols)\n",
    "submission.insert(0, \"ID\", test[\"ID\"])\n",
    "submission_path = \"/mnt/data/submission_svr_only_fixed.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "submission_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fb5d0b",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Extract targets\n",
    "target_cols = [col for col in train.columns if \"BlendProperty\" in col]\n",
    "\n",
    "# Handle absence of ID in train\n",
    "drop_cols = target_cols.copy()\n",
    "if \"ID\" in train.columns:\n",
    "    drop_cols.insert(0, \"ID\")\n",
    "X = train.drop(columns=drop_cols)\n",
    "y = train[target_cols]\n",
    "\n",
    "# Test features\n",
    "X_test = test.drop(columns=[\"ID\"]) if \"ID\" in test.columns else test.copy()\n",
    "\n",
    "# Fill missing values and ensure float\n",
    "X = X.fillna(0).astype(float)\n",
    "X_test = X_test.fillna(0).astype(float)\n",
    "y = y.fillna(0).astype(float)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# K-Fold CV\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "svr_preds = np.zeros((X_test.shape[0], len(target_cols)))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_scaled)):\n",
    "    print(f\"Fold {fold+1}\")\n",
    "    X_train, y_train = X_scaled[train_idx], y.iloc[train_idx]\n",
    "\n",
    "    svr = MultiOutputRegressor(SVR(kernel='rbf', C=1.0, epsilon=0.1))\n",
    "    svr.fit(X_train, y_train)\n",
    "    svr_preds += svr.predict(X_test_scaled)\n",
    "\n",
    "# Average predictions\n",
    "svr_preds /= kf.get_n_splits()\n",
    "\n",
    "# Save submission\n",
    "submission = pd.DataFrame(svr_preds, columns=target_cols)\n",
    "submission.insert(0, \"ID\", test[\"ID\"] if \"ID\" in test.columns else np.arange(1, len(test)+1))\n",
    "submission_path = \"C:\\\\Users\\\\USER\\\\Desktop\\\\shell.ai\\\\submission_svr.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "submission_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7a0f40",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Extract targets\n",
    "target_cols = [col for col in train.columns if \"BlendProperty\" in col]\n",
    "\n",
    "# Drop unused columns\n",
    "drop_cols = target_cols.copy()\n",
    "if \"ID\" in train.columns:\n",
    "    drop_cols.insert(0, \"ID\")\n",
    "X = train.drop(columns=drop_cols)\n",
    "y = train[target_cols]\n",
    "\n",
    "X_test = test.drop(columns=[\"ID\"]) if \"ID\" in test.columns else test.copy()\n",
    "\n",
    "# Preprocessing\n",
    "X = X.fillna(0).astype(float)\n",
    "X_test = X_test.fillna(0).astype(float)\n",
    "y = y.fillna(0).astype(float)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "lr_preds = np.zeros((X_test.shape[0], len(target_cols)))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_scaled)):\n",
    "    print(f\"Fold {fold+1}\")\n",
    "    X_train, y_train = X_scaled[train_idx], y.iloc[train_idx]\n",
    "\n",
    "    model = MultiOutputRegressor(LinearRegression())\n",
    "    model.fit(X_train, y_train)\n",
    "    lr_preds += model.predict(X_test_scaled)\n",
    "\n",
    "lr_preds /= kf.get_n_splits()\n",
    "\n",
    "# Prepare submission\n",
    "submission = pd.DataFrame(lr_preds, columns=target_cols)\n",
    "submission.insert(0, \"ID\", test[\"ID\"] if \"ID\" in test.columns else np.arange(1, len(test)+1))\n",
    "submission_path = \"C:\\\\Users\\\\USER\\\\Desktop\\\\shell.ai\\\\submission_linear_regression.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "submission_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b00ccc",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Extract target columns and features\n",
    "target_cols = [col for col in train.columns if \"BlendProperty\" in col]\n",
    "X = train.drop(columns=target_cols)\n",
    "y = train[target_cols]\n",
    "X_test = test.copy()\n",
    "\n",
    "# Define model\n",
    "base_model = XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.03,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    tree_method=\"hist\"\n",
    ")\n",
    "model = MultiOutputRegressor(base_model)\n",
    "\n",
    "# Cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "y_preds = np.zeros((len(X_test), len(target_cols)))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"Training fold {fold + 1}...\")\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_preds += model.predict(X_test) / kf.n_splits\n",
    "\n",
    "# Save submission\n",
    "submission = pd.DataFrame(y_preds, columns=target_cols)\n",
    "submission.insert(0, \"ID\", test[\"ID\"] if \"ID\" in test.columns else range(1, len(test)+1))\n",
    "submission.to_csv(\"submission_xgboost.csv\", index=False)\n",
    "print(\"Saved: submission_xgboost.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0546b2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50586601",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv(\"train.csv\")  # Update path if needed\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Define target columns\n",
    "target_cols = [col for col in train.columns if \"BlendProperty\" in col]\n",
    "\n",
    "# Features and target\n",
    "X = train.drop(columns=[\"ID\"] + target_cols, errors=\"ignore\")\n",
    "y = train[target_cols]\n",
    "X_test = test.drop(columns=[\"ID\"], errors=\"ignore\")\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Prepare predictions container\n",
    "et_preds = np.zeros((X_test.shape[0], len(target_cols)))\n",
    "\n",
    "# Cross-validation loop\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"🌿 Fold {fold+1}\")\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    model = MultiOutputRegressor(\n",
    "        ExtraTreesRegressor(\n",
    "            n_estimators=500,\n",
    "            max_depth=20,\n",
    "            min_samples_split=4,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    et_preds += model.predict(X_test)\n",
    "\n",
    "# Average over folds\n",
    "et_preds /= kf.get_n_splits()\n",
    "\n",
    "# Prepare submission\n",
    "submission = pd.DataFrame(et_preds, columns=target_cols)\n",
    "submission.insert(0, \"ID\", test[\"ID\"])\n",
    "submission.to_csv(\"submission_extratrees.csv\", index=False)\n",
    "print(\"✅ Submission saved as 'submission_extratrees.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bbfd20",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "sample_submission = pd.read_csv(\"sample_solution.csv\")\n",
    "\n",
    "# Drop ID if present\n",
    "X = train.drop(columns=[\"ID\"] + [f'BlendProperty{i}' for i in range(1, 11)], errors='ignore')\n",
    "y = train[[f'BlendProperty{i}' for i in range(1, 11)]]\n",
    "X_test = test.drop(columns=[\"ID\"], errors='ignore')\n",
    "\n",
    "# Optional: Feature engineering (add more if needed)\n",
    "for i in range(1, 11):\n",
    "    X[f'Property{i}_mean'] = X[[f'Component{j}_Property{i}' for j in range(1, 6)]].mean(axis=1)\n",
    "    X[f'Property{i}_std'] = X[[f'Component{j}_Property{i}' for j in range(1, 6)]].std(axis=1)\n",
    "    X_test[f'Property{i}_mean'] = X_test[[f'Component{j}_Property{i}' for j in range(1, 6)]].mean(axis=1)\n",
    "    X_test[f'Property{i}_std'] = X_test[[f'Component{j}_Property{i}' for j in range(1, 6)]].std(axis=1)\n",
    "\n",
    "# CatBoost parameters\n",
    "catboost_params = {\n",
    "    'loss_function': 'MAPE',\n",
    "    'verbose': 0,\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'iterations': 1000,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'random_seed': 42\n",
    "}\n",
    "\n",
    "# KFold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "y_preds = np.zeros((X_test.shape[0], y.shape[1]))\n",
    "oof_preds = np.zeros_like(y)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"Training Fold {fold + 1}...\")\n",
    "    \n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    model = MultiOutputRegressor(CatBoostRegressor(**catboost_params))\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Validation prediction for internal score\n",
    "    oof_preds[val_idx] = model.predict(X_val)\n",
    "\n",
    "    # Predict on test set\n",
    "    y_preds += model.predict(X_test) / kf.n_splits\n",
    "\n",
    "# Evaluate internal CV score (optional)\n",
    "cv_score = mean_absolute_percentage_error(y, oof_preds)\n",
    "print(f\"CV MAPE Score: {cv_score:.5f}\")\n",
    "\n",
    "# Save submission\n",
    "submission = sample_submission.copy()\n",
    "submission.iloc[:, 1:] = y_preds\n",
    "submission.to_csv(\"submission_catboost_mape.csv\", index=False)\n",
    "print(\"Submission saved to 'submission_catboost_mape.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed83786",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "sample_submission = pd.read_csv(\"sample_solution.csv\")\n",
    "\n",
    "# Drop ID if exists\n",
    "X = train.drop(columns=[\"ID\"] + [f'BlendProperty{i}' for i in range(1, 11)], errors=\"ignore\")\n",
    "y = train[[f'BlendProperty{i}' for i in range(1, 11)]]\n",
    "X_test = test.drop(columns=[\"ID\"], errors=\"ignore\")\n",
    "\n",
    "# -------- FEATURE ENGINEERING --------\n",
    "def add_engineered_features(X_df):\n",
    "    X = X_df.copy()\n",
    "\n",
    "    # 1. Mean & Std of each property across 5 components\n",
    "    for i in range(1, 11):\n",
    "        cols = [f'Component{j}_Property{i}' for j in range(1, 6)]\n",
    "        X[f'Property{i}_mean'] = X[cols].mean(axis=1)\n",
    "        X[f'Property{i}_std'] = X[cols].std(axis=1)\n",
    "\n",
    "    # 2. Weighted Average (blend-weighted mean for each property)\n",
    "    for i in range(1, 11):\n",
    "        blend_weighted = sum(\n",
    "            X[f'Component{j}_fraction'] * X[f'Component{j}_Property{i}'] for j in range(1, 6)\n",
    "        )\n",
    "        X[f'Property{i}_blend_weighted'] = blend_weighted\n",
    "\n",
    "    # 3. Interaction Features (non-linear)\n",
    "    X['frac1_frac2'] = X['Component1_fraction'] * X['Component2_fraction']\n",
    "    X['frac3_frac5'] = X['Component3_fraction'] * X['Component5_fraction']\n",
    "    X['prop1_mean_x_prop2_mean'] = X['Property1_mean'] * X['Property2_mean']\n",
    "    X['std1_x_std2'] = X['Property1_std'] * X['Property2_std']\n",
    "\n",
    "    return X\n",
    "\n",
    "X = add_engineered_features(X)\n",
    "X_test = add_engineered_features(X_test)\n",
    "\n",
    "# -------- LIGHTGBM MODEL SETUP --------\n",
    "lgbm_params = {\n",
    "    'learning_rate': 0.03,\n",
    "    'n_estimators': 1000,\n",
    "    'max_depth': 7,\n",
    "    'num_leaves': 31,\n",
    "    'objective': 'mape',\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "y_preds = np.zeros((X_test.shape[0], y.shape[1]))\n",
    "oof_preds = np.zeros_like(y)\n",
    "\n",
    "print(\"Training with 5-fold CV...\")\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"Fold {fold+1}...\")\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    model = MultiOutputRegressor(lgb.LGBMRegressor(**lgbm_params))\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    oof_preds[val_idx] = model.predict(X_val)\n",
    "    y_preds += model.predict(X_test) / kf.n_splits\n",
    "\n",
    "# -------- EVALUATION --------\n",
    "cv_score = mean_absolute_percentage_error(y, oof_preds)\n",
    "print(f\"CV MAPE Score: {cv_score:.5f}\")\n",
    "\n",
    "# -------- SHAP FEATURE IMPORTANCE (on 1 target model) --------\n",
    "explainer = shap.TreeExplainer(model.estimators_[0])\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "shap.summary_plot(shap_values, X, plot_type=\"bar\", max_display=20)\n",
    "plt.show()\n",
    "\n",
    "# -------- DROP WEAK FEATURES (Optional: SHAP threshold < 0.005) --------\n",
    "shap_importances = np.abs(shap_values).mean(axis=0)\n",
    "important_features = X.columns[shap_importances > 0.005]\n",
    "X = X[important_features]\n",
    "X_test = X_test[important_features]\n",
    "\n",
    "# -------- RETRAIN ON SELECTED FEATURES --------\n",
    "print(\"Retraining after SHAP feature selection...\")\n",
    "y_preds = np.zeros((X_test.shape[0], y.shape[1]))\n",
    "oof_preds = np.zeros_like(y)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    model = MultiOutputRegressor(lgb.LGBMRegressor(**lgbm_params))\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    oof_preds[val_idx] = model.predict(X_val)\n",
    "    y_preds += model.predict(X_test) / kf.n_splits\n",
    "\n",
    "# Final CV score after SHAP filtering\n",
    "cv_score_shap = mean_absolute_percentage_error(y, oof_preds)\n",
    "print(f\"CV MAPE after SHAP selection: {cv_score_shap:.5f}\")\n",
    "\n",
    "# -------- SUBMISSION --------\n",
    "submission = sample_submission.copy()\n",
    "submission.iloc[:, 1:] = y_preds\n",
    "submission.to_csv(\"submission_lgbm_features_shap.csv\", index=False)\n",
    "print(\"Submission saved as 'submission_lgbm_features_shap.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d0f3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user numpy optuna tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39f84c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"loss_function\": \"MAE\",  # MAPE is unstable; CatBoost recommends MAE for robustness\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 300, 1000),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1.0, 10.0),\n",
    "        \"random_strength\": trial.suggest_float(\"random_strength\", 0.1, 1.0),\n",
    "        \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0, 1.0),\n",
    "        \"early_stopping_rounds\": 50,\n",
    "        \"verbose\": 0\n",
    "    }\n",
    "\n",
    "    # KFold on first blend target only for speed\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    X = X_reduced\n",
    "    y = train[[col for col in train.columns if \"BlendProperty\" in col]].iloc[:, 0]\n",
    "\n",
    "    scores = []\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        model = CatBoostRegressor(**params)\n",
    "        model.fit(X_tr, y_tr, eval_set=(X_val, y_val), use_best_model=True)\n",
    "\n",
    "        preds = model.predict(X_val)\n",
    "        score = mean_absolute_percentage_error(y_val, preds)\n",
    "        scores.append(score)\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "# Start tuning\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best params:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02beb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Use SHAP-reduced features\n",
    "X = X_reduced.copy()\n",
    "y = train[[col for col in train.columns if \"BlendProperty\" in col]].iloc[:, 0]  # First target only\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"loss_function\": \"MAE\",\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 300, 1000),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1.0, 10.0),\n",
    "        \"random_strength\": trial.suggest_float(\"random_strength\", 0.1, 1.0),\n",
    "        \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0, 1.0),\n",
    "        \"early_stopping_rounds\": 50,\n",
    "        \"verbose\": 0\n",
    "    }\n",
    "\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        model = CatBoostRegressor(**params)\n",
    "        model.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=True)\n",
    "        preds = model.predict(X_val)\n",
    "        score = mean_absolute_percentage_error(y_val, preds)\n",
    "        scores.append(score)\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "# Run Optuna\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# Best params\n",
    "print(\"Best MAPE:\", study.best_value)\n",
    "print(\"Best params:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc88715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow scikit-learn pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc596e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load datasets\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "# Drop ID column if present\n",
    "X = train.drop(columns=[\"ID\"] + [f'BlendProperty{i}' for i in range(1, 11)], errors='ignore')\n",
    "y = train[[f'BlendProperty{i}' for i in range(1, 11)]]\n",
    "X_test = test.drop(columns=[\"ID\"], errors='ignore')\n",
    "\n",
    "# -------- FEATURE ENGINEERING FUNCTION --------\n",
    "def add_engineered_features(X_df):\n",
    "    X = X_df.copy()\n",
    "\n",
    "    # Mean and Std per property\n",
    "    for i in range(1, 11):\n",
    "        prop_cols = [f'Component{j}_Property{i}' for j in range(1, 6)]\n",
    "        X[f'Property{i}_mean'] = X[prop_cols].mean(axis=1)\n",
    "        X[f'Property{i}_std'] = X[prop_cols].std(axis=1)\n",
    "\n",
    "    # Blend-weighted features\n",
    "    for i in range(1, 11):\n",
    "        weighted_sum = sum(X[f'Component{j}_fraction'] * X[f'Component{j}_Property{i}'] for j in range(1, 6))\n",
    "        X[f'Property{i}_blend_weighted'] = weighted_sum\n",
    "\n",
    "    # Non-linear interaction features\n",
    "    X['frac1_frac2'] = X['Component1_fraction'] * X['Component2_fraction']\n",
    "    X['frac3_frac5'] = X['Component3_fraction'] * X['Component5_fraction']\n",
    "    X['prop1_mean_x_prop2_mean'] = X['Property1_mean'] * X['Property2_mean']\n",
    "    X['std1_x_std2'] = X['Property1_std'] * X['Property2_std']\n",
    "\n",
    "    return X\n",
    "\n",
    "# Apply feature engineering\n",
    "X = add_engineered_features(X)\n",
    "X_test = add_engineered_features(X_test)\n",
    "\n",
    "# -------- Keras MAPE Loss --------\n",
    "def mape_loss(y_true, y_pred):\n",
    "    return K.mean(K.abs((y_true - y_pred) / K.clip(K.abs(y_true), K.epsilon(), None)))\n",
    "\n",
    "# -------- Model Architecture --------\n",
    "def build_model(input_dim, output_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=input_dim, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(output_dim))  # Linear output layer\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss=mape_loss)\n",
    "    return model\n",
    "\n",
    "# -------- 5-Fold Cross Validation --------\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "y_preds = np.zeros((X_test.shape[0], y.shape[1]))\n",
    "oof_preds = np.zeros_like(y)\n",
    "\n",
    "print(\"Training ANN with 5-Fold CV...\")\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"Fold {fold+1}...\")\n",
    "    X_train, X_val = X.iloc[train_idx].values, X.iloc[val_idx].values\n",
    "    y_train, y_val = y.iloc[train_idx].values, y.iloc[val_idx].values\n",
    "\n",
    "    model = build_model(input_dim=X.shape[1], output_dim=y.shape[1])\n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=150, batch_size=32, verbose=0)\n",
    "\n",
    "    oof_preds[val_idx] = model.predict(X_val)\n",
    "    y_preds += model.predict(X_test.values) / kf.n_splits\n",
    "\n",
    "# -------- Evaluation --------\n",
    "cv_score = mean_absolute_percentage_error(y, oof_preds)\n",
    "print(f\"CV MAPE Score (ANN): {cv_score:.5f}\")\n",
    "\n",
    "# -------- Submission --------\n",
    "submission = sample_submission.copy()\n",
    "submission.iloc[:, 1:] = y_preds\n",
    "submission.to_csv(\"submission_ann_engineered.csv\", index=False)\n",
    "print(\"Saved: submission_ann_engineered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f502c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ca4d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
